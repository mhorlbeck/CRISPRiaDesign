
1. Learning sgRNA predictors from empirical data
    * Load scripts and empirical data
    * Generate TSS annotation using FANTOM dataset
    * Calculate parameters for empirical sgRNAs
    * Fit parameters
2. Applying machine learning model to predict sgRNA activity
    * Find all sgRNAs in genomic regions of interest 
    * Predicting sgRNA activity
3. Construct sgRNA libraries
    * Score sgRNAs for off-target potential
* Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering
* Design negative controls matching the base composition of the library
* Finalizing library design

# 1. Learning sgRNA predictors from empirical data
## Load scripts and empirical data


```python
%run sgRNA_learning.py
```


```python
genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)
gencodeData = loadGencodeData(GTF_FILE_FROM_GENCODE)
```


```python
#load empirical data as tables in the format generated by github.com/mhorlbeck/ScreenProcessing
libraryTable, phenotypeTable, geneTable = loadExperimentData(PATHS_TO_DATA_GENERATED_BY_ScreenProcessing)
```


```python
#extract genes that scored as hits, normalize phenotypes, and extract information on sgRNAs from the sgIDs
discriminantTable = calculateDiscriminantScores(geneTable)
normedScores, maxDiscriminantTable = getNormalizedsgRNAsOverThresh(libraryTable, phenotypeTable, discriminantTable, 
                                                                   DISCRIMANT_THRESHOLD_eg20,
                                                                   3, transcripts=False)

libraryTable_subset = libraryTable.loc[normedScores.dropna().index]
sgInfoTable = parseAllSgIds(libraryTable_subset)
```

## Generate TSS annotation using FANTOM dataset


```python
#first generates a table of TSS annotations
#legacy function to make an intermediate table for the "P1P2" annotation strategy, will be replaced in future versions
#TSS_TABLE_BASED_ON_ENSEMBL is table without headers with columns:
#gene, transcript, chromosome, TSS coordinate, strand, annotation_source(optional)
tssTable = generateTssTable(geneTable, TSS_TABLE_BASED_ON_ENSEMBL, FANTOM_TSS_ANNOTATION_BED, 200)
```


```python
#Now create a TSS annotation by searching for P1 and P2 peaks near annotated TSSs
geneToAliases = generateAliasDict(HGNC_SYMBOL_LOOKUP_TABLE,gencodeData)
p1p2Table = generateTssTable_P1P2strategy(tssTable.loc[tssTable.apply(lambda row: row.name[0][:6] != 'pseudo',axis=1)],
                                          FANTOM_TSS_ANNOTATION_BED, 30000, 500, 200, 1000, geneToAliases[0])
#the function will report some collisions of IDs due to use of aliases and redundancy in genome, but will resolve these itself
```


```python
#save tables for downstream use
tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\t', index_col=range(2))
p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\t', header=0, index_col=range(2))
```

## Calculate parameters for empirical sgRNAs


```python
#Load bigwig files for any chromatin data of interest
bwhandleDict = {'dnase':BigWigFile(open('ENCODE_data/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig')),
'faire':BigWigFile(open('ENCODE_data/wgEncodeOpenChromFaireK562Sig.bigWig')),
'mnase':BigWigFile(open('ENCODE_data/wgEncodeSydhNsomeK562Sig.bigWig'))}
```


```python
paramTable_trainingGuides = generateTypicalParamTable(libraryTable_subset,sgInfoTable, tssTable, p1p2Table, genomeDict, bwhandleDict)
```

## Fit parameters


```python
#populate table of fitting parameters
typeList = ['binnable_onehot', 
            'continuous', 'continuous', 'continuous', 'continuous',
            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',
            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',
            'binary']
typeList.extend(['binary']*160)
typeList.extend(['binary']*(16*38))
typeList.extend(['binnable_onehot']*3)
typeList.extend(['binnable_onehot']*2)
typeList.extend(['binary']*18)
fitTable = pd.DataFrame(typeList, index=paramTable_trainingGuides.columns, columns=['type'])
fitparams =[{'bin width':1, 'min edge data':50, 'bin function':np.median},
            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},
            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},
            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},
            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},
            {'bin width':1, 'min edge data':50, 'bin function':np.median},
            {'bin width':1, 'min edge data':50, 'bin function':np.median},
            {'bin width':1, 'min edge data':50, 'bin function':np.median},
            {'bin width':1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},
            {'bin width':.1, 'min edge data':50, 'bin function':np.median},dict()]
fitparams.extend([dict()]*160)
fitparams.extend([dict()]*(16*38))
fitparams.extend([
            {'bin width':.15, 'min edge data':50, 'bin function':np.median},
            {'bin width':.15, 'min edge data':50, 'bin function':np.median},
            {'bin width':.15, 'min edge data':50, 'bin function':np.median}])
fitparams.extend([
            {'bin width':2, 'min edge data':50, 'bin function':np.median},
            {'bin width':2, 'min edge data':50, 'bin function':np.median}])
fitparams.extend([dict()]*18)
fitTable['params'] = fitparams
```


```python
#divide empirical data into n-folds for cross-validation
geneFoldList = getGeneFolds(libraryTable_subset, 5, transcripts=False)
```


```python
#for each fold, fit parameters to training folds and measure ROC on test fold
coefs = []
scoreTups = []
transformedParamTups = []

for geneFold_train, geneFold_test in geneFoldList:

    transformedParams_train, estimators = fitParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_train], normedScores.loc[normedScores.dropna().index].iloc[geneFold_train], fitTable)

    transformedParams_test = transformParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_test], fitTable, estimators)
    
    reg = linear_model.ElasticNetCV(l1_ratio=[.5, .75, .9, .99,1], n_jobs=16, max_iter=2000)
    
    scaler = preprocessing.StandardScaler()
    reg.fit(scaler.fit_transform(transformedParams_train), normedScores.loc[normedScores.dropna().index].iloc[geneFold_train])
    predictedScores = pd.Series(reg.predict(scaler.transform(transformedParams_test)), index=transformedParams_test.index)
    testScores = normedScores.loc[normedScores.dropna().index].iloc[geneFold_test]
    
    transformedParamTups.append((scaler.transform(transformedParams_train),scaler.transform(transformedParams_test)))
    scoreTups.append((testScores, predictedScores))
    
    print 'Prediction AUC-ROC:', metrics.roc_auc_score((testScores >= .75).values, np.array(predictedScores.values,dtype='float64'))
    print 'Prediction R^2:', reg.score(scaler.transform(transformedParams_test), testScores)
    print 'Regression parameters:', reg.l1_ratio_, reg.alpha_
    coefs.append(pd.DataFrame(zip(*[abs(reg.coef_),reg.coef_]), index = transformedParams_test.columns, columns=['abs','true']))
    print 'Number of features used:', len(coefs[-1]) - sum(coefs[-1]['abs'] < .00000000001)
```


```python
#can select an arbitrary fold (as shown here simply the last one tested) to save state for reproducing estimators later
#the pickling of the scikit-learn estimators/regressors will allow the model to be reloaded for prediction of other guide designs, 
#   but will not be compatible across scikit-learn versions, so it is important to preserve the training data and training/test folds
import cPickle
estimatorString = cPickle.dumps((fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test)))
with open(PICKLE_FILE,'w') as outfile:
    outfile.write(estimatorString)
    
#also save the transformed parameters as these can slightly differ based on the automated binning strategy
transformedParams_train.head().to_csv(TRANSFORMED_PARAM_HEADER,sep='\t')
```

# 2. Applying machine learning model to predict sgRNA activity


```python
#starting from a new session for demonstration purposes:
%run sgRNA_learning.py
import cPickle

#load tssTable, p1p2Table, genome sequence, chromatin data
tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\t', index_col=range(2))

p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\t', header=0, index_col=range(2))
p1p2Table['primary TSS'] = p1p2Table['primary TSS'].apply(lambda tupString: (int(tupString.strip('()').split(', ')[0]), int(tupString.strip('()').split(', ')[1])))
p1p2Table['secondary TSS'] = p1p2Table['secondary TSS'].apply(lambda tupString: (int(tupString.strip('()').split(', ')[0]),int(tupString.strip('()').split(', ')[1])))

genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)

bwhandleDict = {'dnase':BigWigFile(open('ENCODE_data/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig')),
'faire':BigWigFile(open('ENCODE_data/wgEncodeOpenChromFaireK562Sig.bigWig')),
'mnase':BigWigFile(open('ENCODE_data/wgEncodeSydhNsomeK562Sig.bigWig'))}

#load sgRNA prediction model saved after the parameter fitting step
with open(PICKLE_FILE) as infile:
    fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test) = cPickle.load(infile)
    
transformedParamHeader = pd.read_csv(TRANSFORMED_PARAM_HEADER,sep='\t')
```

## Find all sgRNAs in genomic regions of interest 


```python
#use the same p1p2Table as above or generate a new one for novel TSSs
libraryTable_new, sgInfoTable_new = findAllGuides(p1p2Table, genomeDict, (-25,500))
```


```python
#alternately, load tables of sgRNAs to score:
libraryTable_new = pd.read_csv(LIBRARY_TABLE_PATH,sep='\t',index_col=0)
sgInfoTable_new = pd.read_csv(SGINFO_TABLE_PATH,sep='\t',index_col=0)
```

## Predicting sgRNA activity


```python
#calculate parameters for new sgRNAs
paramTable_new = generateTypicalParamTable(libraryTable_new, sgInfoTable_new, tssTable, p1p2Table, genomeDict, bwhandleDict)
```


```python
#transform and predict scores according to sgRNA prediction model
transformedParams_new = transformParams(paramTable_new, fitTable, estimators)

#reconcile any differences in column headers generated by automated binning
colTups = []
for (l1, l2), col in transformedParams_new.iteritems():
    colTups.append((l1,str(l2)))
transformedParams_new.columns = pd.MultiIndex.from_tuples(colTups)

predictedScores_new = pd.Series(reg.predict(scaler.transform(transformedParams_new.loc[:, transformedParamHeader.columns].fillna(0).values)), index=transformedParams_new.index)
```


```python
predictedScores_new.to_csv(PREDICTED_SCORE_TABLE, sep='\t')
```

# 3. Construct sgRNA libraries
## Score sgRNAs for off-target potential


```python
#There are many ways to score sgRNAs as off-target; below is one listed one method that is simple and flexible,
#but ignores gapped alignments, alternate PAMs, and uses bowtie which may not be maximally sensitive in all cases
```


```python
#output all sequences to a temporary FASTQ file for running bowtie alignment
def outputTempBowtieFastq(libraryTable, outputFileName):
    phredString = 'I4!=======44444+++++++' #weighting for how impactful mismatches are along sgRNA sequence 
    with open(outputFileName,'w') as outfile:
        for name, row in libraryTable.iterrows():
            outfile.write('@' + name + '\n')
            outfile.write('CCN' + str(Seq.Seq(row['sequence'][1:]).reverse_complement()) + '\n')
            outfile.write('+\n')
            outfile.write(phredString + '\n')
            
outputTempBowtieFastq(libraryTable_new, TEMP_FASTQ_FILE)
```


```python
import subprocess
fqFile = TEMP_FASTQ_FILE

#specifying a list of parameters to run bowtie with
#each tuple contains
# *the mismatch threshold below which a site is considered a potential off-target (higher is more stringent)
# *the number of sites allowed (1 is minimum since each sgRNA should have one true site in genome)
# *the genome index against which to align the sgRNA sequences; these can be custom built to only consider sites near TSSs
# *a name for the bowtie run to create appropriately named output files
alignmentList = [(39,1,'~/indices/hg19.ensemblTSSflank500b','39_nearTSS'),
                (31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS'),
                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome'),
                (31,2,'~/indices/hg19.ensemblTSSflank500b','31_2_nearTSS'),
                (31,3,'~/indices/hg19.ensemblTSSflank500b','31_3_nearTSS')]

alignmentColumns = []
for btThreshold, mflag, bowtieIndex, runname in alignmentList:

    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'
    unalignedFile = 'bowtie_output/' + runname + '_unaligned.fq'
    maxFile = 'bowtie_output/' + runname + '_max.fq'
    
    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile
    print bowtieString
    print subprocess.call(bowtieString, shell=True)

    #parse through the file of sgRNAs that exceeded "m", the maximum allowable alignments, and mark "True" any that are found
    with open(maxFile) as infile:
        sgsAligning = set()
        for i, line in enumerate(infile):
            if i%4 == 0: #id line
                sgsAligning.add(line.strip()[1:])

    alignmentColumns.append(libraryTable_new.apply(lambda row: row.name in sgsAligning, axis=1))
    
#collate results into a table, and flip the boolean values to yield the sgRNAs that passed filter as True
alignmentTable = pd.concat(alignmentColumns,axis=1, keys=zip(*alignmentList)[3]).ne(True)
```

## Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering


```python
#combine all generated data into one master table
predictedScores_new.name = 'predicted score'
v2Table = pd.concat((libraryTable_new, predictedScores_new, alignmentTable, sgInfoTable_new), axis=1, keys=['library table v2', 'predicted score', 'off-target filters', 'sgRNA info'])
```


```python
import re
#for our pCRISPRi/a-v2 vector, we append flanking sequences to each sgRNA sequence for cloning and require the oligo to contain
#exactly 1 BstXI and BlpI site each for cloning, and exactly 0 SbfI sites for sequencing sample preparation
restrictionSites = {re.compile('CCA......TGG'):1,
                   re.compile('GCT.AGC'):1,
                   re.compile('CCTGCAGG'):0}

def matchREsites(sequence, REdict):
    seq = sequence.upper()
    for resite, numMatchesExpected in restrictionSites.iteritems():
        if len(resite.findall(seq)) != numMatchesExpected:
            return False
        
    return True

def checkOverlaps(leftPosition, acceptedLeftPositions, nonoverlapMin):
    for pos in acceptedLeftPositions:
        if abs(pos - leftPosition) < nonoverlapMin:
            return False
    return True
```


```python
#flanking sequences
upstreamConstant = 'CCACCTTGTTG'
downstreamConstant = 'GTTTAAGAGCTAAGCTG'

#minimum overlap between two sgRNAs targeting the same TSS
nonoverlapMin = 3

#number of sgRNAs to pick per gene/TSS
sgRNAsToPick = 10

#list of off-target filter (or combinations of filters) levels, matching the names in the alignment table above
offTargetLevels = [['31_nearTSS', '21_genome'],
                  ['31_nearTSS'],
                  ['21_genome'],
                  ['31_2_nearTSS'],
                  ['31_3_nearTSS']]

#for each gene/TSS, go through each sgRNA in descending order of predicted score
#if an sgRNA passes the restriction site, overlap, and off-target filters, accept it into the library
#if the number of sgRNAs accepted is less than sgRNAsToPick, reduce off-target stringency by one and continue
v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])
newSgIds = []
unfinishedTss = []
for (gene, transcript), group in v2Groups:
    geneSgIds = []
    geneLeftPositions = []
    empiricalSgIds = dict()
    
    stringency = 0
    
    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):
        for sgId_v2, row in group.sort(('predicted score','predicted score'), ascending=False).iterrows():
            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant
            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)
            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \
                and matchREsites(oligoSeq, restrictionSites) \
                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):
                geneSgIds.append((sgId_v2,
                                  gene,transcript,
                                  row[('library table v2','sequence')], oligoSeq,
                                  row[('predicted score','predicted score')], np.nan,
                                 stringency))
                geneLeftPositions.append(leftPos)
                
        stringency += 1
            
    if len(geneSgIds) < sgRNAsToPick:
        unfinishedTss.append((gene, transcript)) #if the number of accepted sgRNAs is still less than sgRNAsToPick, discard gene
    else:
        newSgIds.extend(geneSgIds)
        
libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',
 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')
```


```python
#number of sgRNAs accepted at each stringency level
newLibraryTable.groupby('off-target stringency').agg(len).iloc[:,0]
```


```python
#number of TSSs with fewer than required number of sgRNAs (and thus not included in the library)
print len(unfinishedTss)
```


```python
#Note that empirical information from previous screens can be included as well--for example:
geneToDisc = maxDiscriminantTable['best score'].groupby(level=0).agg(max).to_dict()
thresh = 7
empiricalBonus = .2

upstreamConstant = 'CCACCTTGTTG'
downstreamConstant = 'GTTTAAGAGCTAAGCTG'

nonoverlapMin = 3

sgRNAsToPick = 10

offTargetLevels = [['31_nearTSS', '21_genome'],
                  ['31_nearTSS'],
                  ['21_genome'],
                  ['31_2_nearTSS'],
                  ['31_3_nearTSS']]
offTargetLevels_v1 = [[s + '_v1' for s in l] for l in offTargetLevels]

v1Groups = v1Table.groupby([('relative position','gene'),('relative position','transcript')])
v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])

newSgIds = []
unfinishedTss = []
for (gene, transcript), group in v2Groups:
    geneSgIds = []
    geneLeftPositions = []
    empiricalSgIds = dict()
    
    stringency = 0
    
    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):
        
        if gene in geneToDisc and geneToDisc[gene] >= thresh and (gene, transcript) in v1Groups.groups:

            for sgId_v1, row in v1Groups.get_group((gene, transcript)).sort(('Empirical activity score','Empirical activity score'),ascending=False).iterrows():
                oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant
                leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)
                if len(geneSgIds) < sgRNAsToPick and min(abs(row.loc['relative position'].iloc[2:])) < 5000 \
                and row[('Empirical activity score','Empirical activity score')] >= .75 \
                and row['off-target filters'].loc[offTargetLevels_v1[stringency]].all() \
                and matchREsites(oligoSeq, restrictionSites) \
                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):
                    if len(geneSgIds) < 2:
                        geneSgIds.append((row[('library table v2','sgId_v2')],
                                          gene,transcript,
                                          row[('library table v2','sequence')], oligoSeq,
                                          np.nan,row[('Empirical activity score','Empirical activity score')],
                                         stringency))
                        geneLeftPositions.append(leftPos)

                    empiricalSgIds[row[('library table v2','sgId_v2')]] = row[('Empirical activity score','Empirical activity score')]

        adjustedScores = group.apply(lambda row: row[('predicted score','CRISPRiv2 predicted score')] + empiricalBonus if row.name in empiricalSgIds else row[('predicted score','CRISPRiv2 predicted score')], axis=1)
        adjustedScores.name = ('adjusted score','')
        for sgId_v2, row in pd.concat((group,adjustedScores),axis=1).sort(('adjusted score',''), ascending=False).iterrows():
            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant
            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)
            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \
                and matchREsites(oligoSeq, restrictionSites) \
                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):
                geneSgIds.append((sgId_v2,
                                  gene,transcript,
                                  row[('library table v2','sequence')], oligoSeq,
                                  row[('predicted score','CRISPRiv2 predicted score')], empiricalSgIds[sgId_v2] if sgId_v2 in empiricalSgIds else np.nan,
                                 stringency))
                geneLeftPositions.append(leftPos)
                
        stringency += 1
            
    if len(geneSgIds) < sgRNAsToPick:
        unfinishedTss.append((gene, transcript))
    else:
        newSgIds.extend(geneSgIds)

    
libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',
 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')
```

## Design negative controls matching the base composition of the library


```python
#calcluate the base frequency at each position of the sgRNA, then generate random sequences weighted by this frequency
def getBaseFrequencies(libraryTable, baseConversion = {'G':0, 'C':1, 'T':2, 'A':3}):
    baseArray = np.zeros((len(libraryTable),20))

    for i, (index, seq) in enumerate(libraryTable['protospacer sequence'].iteritems()):
        for j, char in enumerate(seq.upper()):
            baseArray[i,j] = baseConversion[char]

    baseTable = pd.DataFrame(baseArray, index = libraryTable.index)
    
    baseFrequencies = baseTable.apply(lambda col: col.groupby(col).agg(len)).fillna(0) / len(baseTable)
    baseFrequencies.index = ['G','C','T','A']
    
    baseCumulativeFrequencies = baseFrequencies.copy()
    baseCumulativeFrequencies.loc['C'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C']
    baseCumulativeFrequencies.loc['T'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C'] + baseFrequencies.loc['T']
    baseCumulativeFrequencies.loc['A'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C'] + baseFrequencies.loc['T'] + baseFrequencies.loc['A']

    return baseFrequencies, baseCumulativeFrequencies

def generateRandomSequence(baseCumulativeFrequencies):
    randArray = np.random.random(baseCumulativeFrequencies.shape[1])
    
    seq = []
    for i, col in baseCumulativeFrequencies.iteritems():
        for base, freq in col.iteritems():
            if randArray[i] < freq:
                seq.append(base)
                break
                
    return ''.join(seq)
```


```python
baseCumulativeFrequencies = getBaseFrequencies(libraryTable_complete)[1]
negList = []
for i in range(30000):
    negList.append(generateRandomSequence(baseCumulativeFrequencies))
negTable = pd.DataFrame(negList, index=['non-targeting_' + str(i) for i in range(30000)], columns = ['sequence'])

outputTempBowtieFastq(negTable, TEMP_FASTQ_FILE)
```


```python
#similar to targeting sgRNA off-target scoring, but looking for sgRNAs with 0 alignments
fqFile = TEMP_FASTQ_FILE

alignmentList = [(31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS_negs'),
                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome_negs')]

alignmentColumns = []
for btThreshold, mflag, bowtieIndex, runname in alignmentList:

    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'
    unalignedFile = 'bowtie_output//' + runname + '_unaligned.fq'
    maxFile = 'bowtie_output/' + runname + '_max.fq'
    
    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile
    print bowtieString
    print subprocess.call(bowtieString, shell=True)

    #read unaligned file for negs, and then don't flip boolean of alignmentTable
    with open(unalignedFile) as infile:
        sgsAligning = set()
        for i, line in enumerate(infile):
            if i%4 == 0: #id line
                sgsAligning.add(line.strip()[1:])

    alignmentColumns.append(negTable.apply(lambda row: row.name in sgsAligning, axis=1))
    
alignmentTable = pd.concat(alignmentColumns,axis=1, keys=zip(*alignmentList)[3])
alignmentTable.head()
```


```python
acceptedNegList = []
negCount = 0
for i, (name, row) in enumerate(pd.concat((negTable,alignmentTable),axis=1, keys=['seq','alignment']).iterrows()):
    oligo = upstreamConstant + row['seq','sequence'] + downstreamConstant
    if row['alignment'].all() and matchREsites(oligo, restrictionSites):
        acceptedNegList.append(('non-targeting_%05d' % negCount, 'negative_control', 'na', row['seq','sequence'], oligo, 0))
        negCount += 1
        
acceptedNegs = pd.DataFrame(acceptedNegList, columns = ['sgId', 'gene', 'transcript', 'protospacer sequence', 'oligo sequence', 'off-target stringency']).set_index('sgId')
```

## Finalizing library design

* divide genes into sublibrary groups (if required)
* assign negative control sgRNAs to sublibrary groups; ~1-2% of the number of sgRNAs in the library is a good rule-of-thumb
* append PCR adapter sequences (~18bp) to each end of the oligo sequences to enable amplification of the oligo pool; each sublibary should have an orthogonal sequence so they can be cloned separately


```python

```
