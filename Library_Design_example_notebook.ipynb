{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Learning sgRNA predictors from empirical data\n",
    "    * Load scripts and empirical data\n",
    "    * Generate TSS annotation using FANTOM dataset\n",
    "    * Calculate parameters for empirical sgRNAs\n",
    "    * Fit parameters\n",
    "2. Applying machine learning model to predict sgRNA activity\n",
    "    * Find all sgRNAs in genomic regions of interest \n",
    "    * Predicting sgRNA activity\n",
    "3. Construct sgRNA libraries\n",
    "    * Score sgRNAs for off-target potential\n",
    "* Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering\n",
    "* Design negative controls matching the base composition of the library\n",
    "* Finalizing library design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning sgRNA predictors from empirical data\n",
    "## Load scripts and empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run sgRNA_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)\n",
    "gencodeData = loadGencodeData(GTF_FILE_FROM_GENCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load empirical data as tables in the format generated by github.com/mhorlbeck/ScreenProcessing\n",
    "libraryTable, phenotypeTable, geneTable = loadExperimentData(PATHS_TO_DATA_GENERATED_BY_ScreenProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract genes that scored as hits, normalize phenotypes, and extract information on sgRNAs from the sgIDs\n",
    "discriminantTable = calculateDiscriminantScores(geneTable)\n",
    "normedScores, maxDiscriminantTable = getNormalizedsgRNAsOverThresh(libraryTable, phenotypeTable, discriminantTable, \n",
    "                                                                   DISCRIMANT_THRESHOLD_eg20,\n",
    "                                                                   3, transcripts=False)\n",
    "\n",
    "libraryTable_subset = libraryTable.loc[normedScores.dropna().index]\n",
    "sgInfoTable = parseAllSgIds(libraryTable_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TSS annotation using FANTOM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first generates a table of TSS annotations\n",
    "#legacy function to make an intermediate table for the \"P1P2\" annotation strategy, will be replaced in future versions\n",
    "#TSS_TABLE_BASED_ON_ENSEMBL is table without headers with columns:\n",
    "#gene, transcript, chromosome, TSS coordinate, strand, annotation_source(optional)\n",
    "tssTable = generateTssTable(geneTable, TSS_TABLE_BASED_ON_ENSEMBL, FANTOM_TSS_ANNOTATION_BED, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now create a TSS annotation by searching for P1 and P2 peaks near annotated TSSs\n",
    "geneToAliases = generateAliasDict(HGNC_SYMBOL_LOOKUP_TABLE,gencodeData)\n",
    "p1p2Table = generateTssTable_P1P2strategy(tssTable.loc[tssTable.apply(lambda row: row.name[0][:6] != 'pseudo',axis=1)],\n",
    "                                          FANTOM_TSS_ANNOTATION_BED, 30000, 500, 200, 1000, geneToAliases[0])\n",
    "#the function will report some collisions of IDs due to use of aliases and redundancy in genome, but will resolve these itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save tables for downstream use\n",
    "tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=range(2))\n",
    "p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate parameters for empirical sgRNAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load bigwig files for any chromatin data of interest\n",
    "bwhandleDict = {'dnase':BigWigFile(open('ENCODE_data/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig')),\n",
    "'faire':BigWigFile(open('ENCODE_data/wgEncodeOpenChromFaireK562Sig.bigWig')),\n",
    "'mnase':BigWigFile(open('ENCODE_data/wgEncodeSydhNsomeK562Sig.bigWig'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramTable_trainingGuides = generateTypicalParamTable(libraryTable_subset,sgInfoTable, tssTable, p1p2Table, genomeDict, bwhandleDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#populate table of fitting parameters\n",
    "typeList = ['binnable_onehot', \n",
    "            'continuous', 'continuous', 'continuous', 'continuous',\n",
    "            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',\n",
    "            'binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot','binnable_onehot',\n",
    "            'binary']\n",
    "typeList.extend(['binary']*160)\n",
    "typeList.extend(['binary']*(16*38))\n",
    "typeList.extend(['binnable_onehot']*3)\n",
    "typeList.extend(['binnable_onehot']*2)\n",
    "typeList.extend(['binary']*18)\n",
    "fitTable = pd.DataFrame(typeList, index=paramTable_trainingGuides.columns, columns=['type'])\n",
    "fitparams =[{'bin width':1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'C':[.01,.05, .1,.5], 'gamma':[.000001, .00005,.0001,.0005]},\n",
    "            {'bin width':1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.1, 'min edge data':50, 'bin function':np.median},dict()]\n",
    "fitparams.extend([dict()]*160)\n",
    "fitparams.extend([dict()]*(16*38))\n",
    "fitparams.extend([\n",
    "            {'bin width':.15, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.15, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':.15, 'min edge data':50, 'bin function':np.median}])\n",
    "fitparams.extend([\n",
    "            {'bin width':2, 'min edge data':50, 'bin function':np.median},\n",
    "            {'bin width':2, 'min edge data':50, 'bin function':np.median}])\n",
    "fitparams.extend([dict()]*18)\n",
    "fitTable['params'] = fitparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#divide empirical data into n-folds for cross-validation\n",
    "geneFoldList = getGeneFolds(libraryTable_subset, 5, transcripts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for each fold, fit parameters to training folds and measure ROC on test fold\n",
    "coefs = []\n",
    "scoreTups = []\n",
    "transformedParamTups = []\n",
    "\n",
    "for geneFold_train, geneFold_test in geneFoldList:\n",
    "\n",
    "    transformedParams_train, estimators = fitParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_train], normedScores.loc[normedScores.dropna().index].iloc[geneFold_train], fitTable)\n",
    "\n",
    "    transformedParams_test = transformParams(paramTable_trainingGuides.loc[normedScores.dropna().index].iloc[geneFold_test], fitTable, estimators)\n",
    "    \n",
    "    reg = linear_model.ElasticNetCV(l1_ratio=[.5, .75, .9, .99,1], n_jobs=16, max_iter=2000)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    reg.fit(scaler.fit_transform(transformedParams_train), normedScores.loc[normedScores.dropna().index].iloc[geneFold_train])\n",
    "    predictedScores = pd.Series(reg.predict(scaler.transform(transformedParams_test)), index=transformedParams_test.index)\n",
    "    testScores = normedScores.loc[normedScores.dropna().index].iloc[geneFold_test]\n",
    "    \n",
    "    transformedParamTups.append((scaler.transform(transformedParams_train),scaler.transform(transformedParams_test)))\n",
    "    scoreTups.append((testScores, predictedScores))\n",
    "    \n",
    "    print 'Prediction AUC-ROC:', metrics.roc_auc_score((testScores >= .75).values, np.array(predictedScores.values,dtype='float64'))\n",
    "    print 'Prediction R^2:', reg.score(scaler.transform(transformedParams_test), testScores)\n",
    "    print 'Regression parameters:', reg.l1_ratio_, reg.alpha_\n",
    "    coefs.append(pd.DataFrame(zip(*[abs(reg.coef_),reg.coef_]), index = transformedParams_test.columns, columns=['abs','true']))\n",
    "    print 'Number of features used:', len(coefs[-1]) - sum(coefs[-1]['abs'] < .00000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#can select an arbitrary fold (as shown here simply the last one tested) to save state for reproducing estimators later\n",
    "#the pickling of the scikit-learn estimators/regressors will allow the model to be reloaded for prediction of other guide designs, \n",
    "#   but will not be compatible across scikit-learn versions, so it is important to preserve the training data and training/test folds\n",
    "import cPickle\n",
    "estimatorString = cPickle.dumps((fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test)))\n",
    "with open(PICKLE_FILE,'w') as outfile:\n",
    "    outfile.write(estimatorString)\n",
    "    \n",
    "#also save the transformed parameters as these can slightly differ based on the automated binning strategy\n",
    "transformedParams_train.head().to_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Applying machine learning model to predict sgRNA activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting from a new session for demonstration purposes:\n",
    "%run sgRNA_learning.py\n",
    "import cPickle\n",
    "\n",
    "#load tssTable, p1p2Table, genome sequence, chromatin data\n",
    "tssTable = pd.read_csv(TSS_TABLE_PATH,sep='\\t', index_col=range(2))\n",
    "\n",
    "p1p2Table = pd.read_csv(P1P2_TABLE_PATH,sep='\\t', header=0, index_col=range(2))\n",
    "p1p2Table['primary TSS'] = p1p2Table['primary TSS'].apply(lambda tupString: (int(tupString.strip('()').split(', ')[0]), int(tupString.strip('()').split(', ')[1])))\n",
    "p1p2Table['secondary TSS'] = p1p2Table['secondary TSS'].apply(lambda tupString: (int(tupString.strip('()').split(', ')[0]),int(tupString.strip('()').split(', ')[1])))\n",
    "\n",
    "genomeDict = loadGenomeAsDict(FASTA_FILE_OF_GENOME)\n",
    "\n",
    "bwhandleDict = {'dnase':BigWigFile(open('ENCODE_data/wgEncodeOpenChromDnaseK562BaseOverlapSignalV2.bigWig')),\n",
    "'faire':BigWigFile(open('ENCODE_data/wgEncodeOpenChromFaireK562Sig.bigWig')),\n",
    "'mnase':BigWigFile(open('ENCODE_data/wgEncodeSydhNsomeK562Sig.bigWig'))}\n",
    "\n",
    "#load sgRNA prediction model saved after the parameter fitting step\n",
    "with open(PICKLE_FILE) as infile:\n",
    "    fitTable, estimators, scaler, reg, (geneFold_train, geneFold_test) = cPickle.load(infile)\n",
    "    \n",
    "transformedParamHeader = pd.read_csv(TRANSFORMED_PARAM_HEADER,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all sgRNAs in genomic regions of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the same p1p2Table as above or generate a new one for novel TSSs\n",
    "libraryTable_new, sgInfoTable_new = findAllGuides(p1p2Table, genomeDict, (-25,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alternately, load tables of sgRNAs to score:\n",
    "libraryTable_new = pd.read_csv(LIBRARY_TABLE_PATH,sep='\\t',index_col=0)\n",
    "sgInfoTable_new = pd.read_csv(SGINFO_TABLE_PATH,sep='\\t',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sgRNA activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate parameters for new sgRNAs\n",
    "paramTable_new = generateTypicalParamTable(libraryTable_new, sgInfoTable_new, tssTable, p1p2Table, genomeDict, bwhandleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform and predict scores according to sgRNA prediction model\n",
    "transformedParams_new = transformParams(paramTable_new, fitTable, estimators)\n",
    "\n",
    "#reconcile any differences in column headers generated by automated binning\n",
    "colTups = []\n",
    "for (l1, l2), col in transformedParams_new.iteritems():\n",
    "    colTups.append((l1,str(l2)))\n",
    "transformedParams_new.columns = pd.MultiIndex.from_tuples(colTups)\n",
    "\n",
    "predictedScores_new = pd.Series(reg.predict(scaler.transform(transformedParams_new.loc[:, transformedParamHeader.columns].fillna(0).values)), index=transformedParams_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedScores_new.to_csv(PREDICTED_SCORE_TABLE, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct sgRNA libraries\n",
    "## Score sgRNAs for off-target potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#There are many ways to score sgRNAs as off-target; below is one listed one method that is simple and flexible,\n",
    "#but ignores gapped alignments, alternate PAMs, and uses bowtie which may not be maximally sensitive in all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output all sequences to a temporary FASTQ file for running bowtie alignment\n",
    "def outputTempBowtieFastq(libraryTable, outputFileName):\n",
    "    phredString = 'I4!=======44444+++++++' #weighting for how impactful mismatches are along sgRNA sequence \n",
    "    with open(outputFileName,'w') as outfile:\n",
    "        for name, row in libraryTable.iterrows():\n",
    "            outfile.write('@' + name + '\\n')\n",
    "            outfile.write('CCN' + str(Seq.Seq(row['sequence'][1:]).reverse_complement()) + '\\n')\n",
    "            outfile.write('+\\n')\n",
    "            outfile.write(phredString + '\\n')\n",
    "            \n",
    "outputTempBowtieFastq(libraryTable_new, TEMP_FASTQ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "fqFile = TEMP_FASTQ_FILE\n",
    "\n",
    "#specifying a list of parameters to run bowtie with\n",
    "#each tuple contains\n",
    "# *the mismatch threshold below which a site is considered a potential off-target (higher is more stringent)\n",
    "# *the number of sites allowed (1 is minimum since each sgRNA should have one true site in genome)\n",
    "# *the genome index against which to align the sgRNA sequences; these can be custom built to only consider sites near TSSs\n",
    "# *a name for the bowtie run to create appropriately named output files\n",
    "alignmentList = [(39,1,'~/indices/hg19.ensemblTSSflank500b','39_nearTSS'),\n",
    "                (31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS'),\n",
    "                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome'),\n",
    "                (31,2,'~/indices/hg19.ensemblTSSflank500b','31_2_nearTSS'),\n",
    "                (31,3,'~/indices/hg19.ensemblTSSflank500b','31_3_nearTSS')]\n",
    "\n",
    "alignmentColumns = []\n",
    "for btThreshold, mflag, bowtieIndex, runname in alignmentList:\n",
    "\n",
    "    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'\n",
    "    unalignedFile = 'bowtie_output/' + runname + '_unaligned.fq'\n",
    "    maxFile = 'bowtie_output/' + runname + '_max.fq'\n",
    "    \n",
    "    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile\n",
    "    print bowtieString\n",
    "    print subprocess.call(bowtieString, shell=True)\n",
    "\n",
    "    #parse through the file of sgRNAs that exceeded \"m\", the maximum allowable alignments, and mark \"True\" any that are found\n",
    "    with open(maxFile) as infile:\n",
    "        sgsAligning = set()\n",
    "        for i, line in enumerate(infile):\n",
    "            if i%4 == 0: #id line\n",
    "                sgsAligning.add(line.strip()[1:])\n",
    "\n",
    "    alignmentColumns.append(libraryTable_new.apply(lambda row: row.name in sgsAligning, axis=1))\n",
    "    \n",
    "#collate results into a table, and flip the boolean values to yield the sgRNAs that passed filter as True\n",
    "alignmentTable = pd.concat(alignmentColumns,axis=1, keys=zip(*alignmentList)[3]).ne(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the top sgRNAs for a library, given predicted activity scores and off-target filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine all generated data into one master table\n",
    "predictedScores_new.name = 'predicted score'\n",
    "v2Table = pd.concat((libraryTable_new, predictedScores_new, alignmentTable, sgInfoTable_new), axis=1, keys=['library table v2', 'predicted score', 'off-target filters', 'sgRNA info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#for our pCRISPRi/a-v2 vector, we append flanking sequences to each sgRNA sequence for cloning and require the oligo to contain\n",
    "#exactly 1 BstXI and BlpI site each for cloning, and exactly 0 SbfI sites for sequencing sample preparation\n",
    "restrictionSites = {re.compile('CCA......TGG'):1,\n",
    "                   re.compile('GCT.AGC'):1,\n",
    "                   re.compile('CCTGCAGG'):0}\n",
    "\n",
    "def matchREsites(sequence, REdict):\n",
    "    seq = sequence.upper()\n",
    "    for resite, numMatchesExpected in restrictionSites.iteritems():\n",
    "        if len(resite.findall(seq)) != numMatchesExpected:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def checkOverlaps(leftPosition, acceptedLeftPositions, nonoverlapMin):\n",
    "    for pos in acceptedLeftPositions:\n",
    "        if abs(pos - leftPosition) < nonoverlapMin:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flanking sequences\n",
    "upstreamConstant = 'CCACCTTGTTG'\n",
    "downstreamConstant = 'GTTTAAGAGCTAAGCTG'\n",
    "\n",
    "#minimum overlap between two sgRNAs targeting the same TSS\n",
    "nonoverlapMin = 3\n",
    "\n",
    "#number of sgRNAs to pick per gene/TSS\n",
    "sgRNAsToPick = 10\n",
    "\n",
    "#list of off-target filter (or combinations of filters) levels, matching the names in the alignment table above\n",
    "offTargetLevels = [['31_nearTSS', '21_genome'],\n",
    "                  ['31_nearTSS'],\n",
    "                  ['21_genome'],\n",
    "                  ['31_2_nearTSS'],\n",
    "                  ['31_3_nearTSS']]\n",
    "\n",
    "#for each gene/TSS, go through each sgRNA in descending order of predicted score\n",
    "#if an sgRNA passes the restriction site, overlap, and off-target filters, accept it into the library\n",
    "#if the number of sgRNAs accepted is less than sgRNAsToPick, reduce off-target stringency by one and continue\n",
    "v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])\n",
    "newSgIds = []\n",
    "unfinishedTss = []\n",
    "for (gene, transcript), group in v2Groups:\n",
    "    geneSgIds = []\n",
    "    geneLeftPositions = []\n",
    "    empiricalSgIds = dict()\n",
    "    \n",
    "    stringency = 0\n",
    "    \n",
    "    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):\n",
    "        for sgId_v2, row in group.sort(('predicted score','predicted score'), ascending=False).iterrows():\n",
    "            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                geneSgIds.append((sgId_v2,\n",
    "                                  gene,transcript,\n",
    "                                  row[('library table v2','sequence')], oligoSeq,\n",
    "                                  row[('predicted score','predicted score')], np.nan,\n",
    "                                 stringency))\n",
    "                geneLeftPositions.append(leftPos)\n",
    "                \n",
    "        stringency += 1\n",
    "            \n",
    "    if len(geneSgIds) < sgRNAsToPick:\n",
    "        unfinishedTss.append((gene, transcript)) #if the number of accepted sgRNAs is still less than sgRNAsToPick, discard gene\n",
    "    else:\n",
    "        newSgIds.extend(geneSgIds)\n",
    "        \n",
    "libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',\n",
    " 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#number of sgRNAs accepted at each stringency level\n",
    "newLibraryTable.groupby('off-target stringency').agg(len).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#number of TSSs with fewer than required number of sgRNAs (and thus not included in the library)\n",
    "print len(unfinishedTss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note that empirical information from previous screens can be included as well--for example:\n",
    "geneToDisc = maxDiscriminantTable['best score'].groupby(level=0).agg(max).to_dict()\n",
    "thresh = 7\n",
    "empiricalBonus = .2\n",
    "\n",
    "upstreamConstant = 'CCACCTTGTTG'\n",
    "downstreamConstant = 'GTTTAAGAGCTAAGCTG'\n",
    "\n",
    "nonoverlapMin = 3\n",
    "\n",
    "sgRNAsToPick = 10\n",
    "\n",
    "offTargetLevels = [['31_nearTSS', '21_genome'],\n",
    "                  ['31_nearTSS'],\n",
    "                  ['21_genome'],\n",
    "                  ['31_2_nearTSS'],\n",
    "                  ['31_3_nearTSS']]\n",
    "offTargetLevels_v1 = [[s + '_v1' for s in l] for l in offTargetLevels]\n",
    "\n",
    "v1Groups = v1Table.groupby([('relative position','gene'),('relative position','transcript')])\n",
    "v2Groups = v2Table.groupby([('library table v2','gene'),('library table v2','transcripts')])\n",
    "\n",
    "newSgIds = []\n",
    "unfinishedTss = []\n",
    "for (gene, transcript), group in v2Groups:\n",
    "    geneSgIds = []\n",
    "    geneLeftPositions = []\n",
    "    empiricalSgIds = dict()\n",
    "    \n",
    "    stringency = 0\n",
    "    \n",
    "    while len(geneSgIds) < sgRNAsToPick and stringency < len(offTargetLevels):\n",
    "        \n",
    "        if gene in geneToDisc and geneToDisc[gene] >= thresh and (gene, transcript) in v1Groups.groups:\n",
    "\n",
    "            for sgId_v1, row in v1Groups.get_group((gene, transcript)).sort(('Empirical activity score','Empirical activity score'),ascending=False).iterrows():\n",
    "                oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "                leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "                if len(geneSgIds) < sgRNAsToPick and min(abs(row.loc['relative position'].iloc[2:])) < 5000 \\\n",
    "                and row[('Empirical activity score','Empirical activity score')] >= .75 \\\n",
    "                and row['off-target filters'].loc[offTargetLevels_v1[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                    if len(geneSgIds) < 2:\n",
    "                        geneSgIds.append((row[('library table v2','sgId_v2')],\n",
    "                                          gene,transcript,\n",
    "                                          row[('library table v2','sequence')], oligoSeq,\n",
    "                                          np.nan,row[('Empirical activity score','Empirical activity score')],\n",
    "                                         stringency))\n",
    "                        geneLeftPositions.append(leftPos)\n",
    "\n",
    "                    empiricalSgIds[row[('library table v2','sgId_v2')]] = row[('Empirical activity score','Empirical activity score')]\n",
    "\n",
    "        adjustedScores = group.apply(lambda row: row[('predicted score','CRISPRiv2 predicted score')] + empiricalBonus if row.name in empiricalSgIds else row[('predicted score','CRISPRiv2 predicted score')], axis=1)\n",
    "        adjustedScores.name = ('adjusted score','')\n",
    "        for sgId_v2, row in pd.concat((group,adjustedScores),axis=1).sort(('adjusted score',''), ascending=False).iterrows():\n",
    "            oligoSeq = upstreamConstant + row[('library table v2','sequence')] + downstreamConstant\n",
    "            leftPos = row[('sgRNA info', 'position')] - (23 if row[('sgRNA info', 'strand')] == '-' else 0)\n",
    "            if len(geneSgIds) < sgRNAsToPick and row['off-target filters'].loc[offTargetLevels[stringency]].all() \\\n",
    "                and matchREsites(oligoSeq, restrictionSites) \\\n",
    "                and checkOverlaps(leftPos, geneLeftPositions, nonoverlapMin):\n",
    "                geneSgIds.append((sgId_v2,\n",
    "                                  gene,transcript,\n",
    "                                  row[('library table v2','sequence')], oligoSeq,\n",
    "                                  row[('predicted score','CRISPRiv2 predicted score')], empiricalSgIds[sgId_v2] if sgId_v2 in empiricalSgIds else np.nan,\n",
    "                                 stringency))\n",
    "                geneLeftPositions.append(leftPos)\n",
    "                \n",
    "        stringency += 1\n",
    "            \n",
    "    if len(geneSgIds) < sgRNAsToPick:\n",
    "        unfinishedTss.append((gene, transcript))\n",
    "    else:\n",
    "        newSgIds.extend(geneSgIds)\n",
    "\n",
    "    \n",
    "libraryTable_complete = pd.DataFrame(newSgIds, columns = ['sgID', 'gene', 'transcript','protospacer sequence', 'oligo sequence',\n",
    " 'predicted score', 'empirical score', 'off-target stringency']).set_index('sgID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design negative controls matching the base composition of the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calcluate the base frequency at each position of the sgRNA, then generate random sequences weighted by this frequency\n",
    "def getBaseFrequencies(libraryTable, baseConversion = {'G':0, 'C':1, 'T':2, 'A':3}):\n",
    "    baseArray = np.zeros((len(libraryTable),20))\n",
    "\n",
    "    for i, (index, seq) in enumerate(libraryTable['protospacer sequence'].iteritems()):\n",
    "        for j, char in enumerate(seq.upper()):\n",
    "            baseArray[i,j] = baseConversion[char]\n",
    "\n",
    "    baseTable = pd.DataFrame(baseArray, index = libraryTable.index)\n",
    "    \n",
    "    baseFrequencies = baseTable.apply(lambda col: col.groupby(col).agg(len)).fillna(0) / len(baseTable)\n",
    "    baseFrequencies.index = ['G','C','T','A']\n",
    "    \n",
    "    baseCumulativeFrequencies = baseFrequencies.copy()\n",
    "    baseCumulativeFrequencies.loc['C'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C']\n",
    "    baseCumulativeFrequencies.loc['T'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C'] + baseFrequencies.loc['T']\n",
    "    baseCumulativeFrequencies.loc['A'] = baseFrequencies.loc['G'] + baseFrequencies.loc['C'] + baseFrequencies.loc['T'] + baseFrequencies.loc['A']\n",
    "\n",
    "    return baseFrequencies, baseCumulativeFrequencies\n",
    "\n",
    "def generateRandomSequence(baseCumulativeFrequencies):\n",
    "    randArray = np.random.random(baseCumulativeFrequencies.shape[1])\n",
    "    \n",
    "    seq = []\n",
    "    for i, col in baseCumulativeFrequencies.iteritems():\n",
    "        for base, freq in col.iteritems():\n",
    "            if randArray[i] < freq:\n",
    "                seq.append(base)\n",
    "                break\n",
    "                \n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseCumulativeFrequencies = getBaseFrequencies(libraryTable_complete)[1]\n",
    "negList = []\n",
    "for i in range(30000):\n",
    "    negList.append(generateRandomSequence(baseCumulativeFrequencies))\n",
    "negTable = pd.DataFrame(negList, index=['non-targeting_' + str(i) for i in range(30000)], columns = ['sequence'])\n",
    "\n",
    "outputTempBowtieFastq(negTable, TEMP_FASTQ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similar to targeting sgRNA off-target scoring, but looking for sgRNAs with 0 alignments\n",
    "fqFile = TEMP_FASTQ_FILE\n",
    "\n",
    "alignmentList = [(31,1,'~/indices/hg19.ensemblTSSflank500b','31_nearTSS_negs'),\n",
    "                (21,1,'~/indices/hg19.maskChrMandPAR','21_genome_negs')]\n",
    "\n",
    "alignmentColumns = []\n",
    "for btThreshold, mflag, bowtieIndex, runname in alignmentList:\n",
    "\n",
    "    alignedFile = 'bowtie_output/' + runname + '_aligned.txt'\n",
    "    unalignedFile = 'bowtie_output//' + runname + '_unaligned.fq'\n",
    "    maxFile = 'bowtie_output/' + runname + '_max.fq'\n",
    "    \n",
    "    bowtieString = 'bowtie -n 3 -l 15 -e '+str(btThreshold)+' -m ' + str(mflag) + ' --nomaqround -a --tryhard -p 16 --chunkmbs 256 ' + bowtieIndex + ' --suppress 5,6,7 --un ' + unalignedFile + ' --max ' + maxFile + ' '+ ' -q '+fqFile+' '+ alignedFile\n",
    "    print bowtieString\n",
    "    print subprocess.call(bowtieString, shell=True)\n",
    "\n",
    "    #read unaligned file for negs, and then don't flip boolean of alignmentTable\n",
    "    with open(unalignedFile) as infile:\n",
    "        sgsAligning = set()\n",
    "        for i, line in enumerate(infile):\n",
    "            if i%4 == 0: #id line\n",
    "                sgsAligning.add(line.strip()[1:])\n",
    "\n",
    "    alignmentColumns.append(negTable.apply(lambda row: row.name in sgsAligning, axis=1))\n",
    "    \n",
    "alignmentTable = pd.concat(alignmentColumns,axis=1, keys=zip(*alignmentList)[3])\n",
    "alignmentTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acceptedNegList = []\n",
    "negCount = 0\n",
    "for i, (name, row) in enumerate(pd.concat((negTable,alignmentTable),axis=1, keys=['seq','alignment']).iterrows()):\n",
    "    oligo = upstreamConstant + row['seq','sequence'] + downstreamConstant\n",
    "    if row['alignment'].all() and matchREsites(oligo, restrictionSites):\n",
    "        acceptedNegList.append(('non-targeting_%05d' % negCount, 'negative_control', 'na', row['seq','sequence'], oligo, 0))\n",
    "        negCount += 1\n",
    "        \n",
    "acceptedNegs = pd.DataFrame(acceptedNegList, columns = ['sgId', 'gene', 'transcript', 'protospacer sequence', 'oligo sequence', 'off-target stringency']).set_index('sgId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing library design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* divide genes into sublibrary groups (if required)\n",
    "* assign negative control sgRNAs to sublibrary groups; ~1-2% of the number of sgRNAs in the library is a good rule-of-thumb\n",
    "* append PCR adapter sequences (~18bp) to each end of the oligo sequences to enable amplification of the oligo pool; each sublibary should have an orthogonal sequence so they can be cloned separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
